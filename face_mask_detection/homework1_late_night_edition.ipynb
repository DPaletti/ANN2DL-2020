{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "homework1_late_night_edition.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GefsMXhCjyXP"
      },
      "source": [
        "#  Setup\n",
        "1. Jupyter Environment Setup\n",
        "2. Dataset Split in ImageDataGenerator format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbfmlVWsjyXX"
      },
      "source": [
        "colab = True # set this to true and activate next cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USXk3TcBjyXr"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp \"/content/drive/My Drive/artificial-neural-networks-and-deep-learning-2020.zip\" .\n",
        "!mkdir Results\n",
        "!cp \"/content/drive/My Drive/tlX.h5\" ./Results\n",
        "!unzip -q artificial-neural-networks-and-deep-learning-2020.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPrwVCPrjyXt"
      },
      "source": [
        "# Cell output set up for Jupyter\n",
        "from pathlib import Path\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8mifj-KjyX3"
      },
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "\n",
        "test_pictures_n = 1684 # 70/30 ratio for training and validation\n",
        "target_file_name = \"train_gt.json\"\n",
        "dataset_name = \"MaskDataset\"\n",
        "\n",
        "# Setting up directory structure\n",
        "Path().joinpath(dataset_name, \"validation\").mkdir(parents=True, exist_ok=True)\n",
        "Path().joinpath(dataset_name, \"training\", \"NO_PERSON\").mkdir(parents=True, exist_ok=True)\n",
        "Path().joinpath(dataset_name, \"training\", \"ALL_THE_PEOPLE\").mkdir(parents=True, exist_ok=True)\n",
        "Path().joinpath(dataset_name, \"training\", \"SOMEONE\").mkdir(parents=True, exist_ok=True)\n",
        "Path().joinpath(dataset_name, \"validation\", \"NO_PERSON\").mkdir(parents=True, exist_ok=True)\n",
        "Path().joinpath(dataset_name, \"validation\", \"ALL_THE_PEOPLE\").mkdir(parents=True, exist_ok=True)\n",
        "Path().joinpath(dataset_name, \"validation\", \"SOMEONE\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Files are moved from the training directory to the corresponding folders\n",
        "# both for training and for validation\n",
        "with open(str(Path().joinpath(dataset_name, target_file_name))) as f:\n",
        "    data = json.load(f)\n",
        "    pictures = list(data.keys())\n",
        "    random.shuffle(pictures)\n",
        "    validation_pictures = pictures[0:test_pictures_n]\n",
        "    for path in Path().joinpath(dataset_name, \"training\").glob(\"*.jpg\"):\n",
        "        if path.name in validation_pictures:\n",
        "            file_destination = str(Path().joinpath(dataset_name, \"validation\", path.name))\n",
        "            path.rename(file_destination)\n",
        "            path = Path(file_destination)\n",
        "        if data[path.name] == 0:\n",
        "            path.rename(str(Path(path.parent).joinpath(\"NO_PERSON\", path.name)))\n",
        "        elif data[path.name] == 1:\n",
        "            path.rename(str(Path(path.parent).joinpath(\"ALL_THE_PEOPLE\", path.name)))\n",
        "        elif data[path.name] == 2:\n",
        "            path.rename(str(Path(path.parent).joinpath(\"SOMEONE\", path.name)))\n",
        "        else:\n",
        "            raise ValueError(\"Unrecognized label in \" + target_file_name + \" allowed values are 0, 1, 2 found: \" + str(data[path.name]))\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJMJDpyYjyYA"
      },
      "source": [
        "# Imports and Random seed setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BkaP8GijyYC"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D, DepthwiseConv2D, LeakyReLU\n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "import scipy as sp\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "tf.random.set_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41tFMUvNjyYH"
      },
      "source": [
        "# Dataset setup: augmentation and batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6YuN_dHjyYI",
        "outputId": "5a00396a-f14c-437f-eb7b-365f4102fe49"
      },
      "source": [
        "# Batch Size\n",
        "bs = 32\n",
        "\n",
        "# Target Image Shape (max 358 x 256)\n",
        "img_h = 256\n",
        "img_w = 256\n",
        "\n",
        "# this is the augmentation configuration we will use for training\n",
        "train_data_gen = ImageDataGenerator(\n",
        "            rescale=1./255,\n",
        "            rotation_range=10,\n",
        "            zoom_range=1,\n",
        "            width_shift_range=0.1, \n",
        "            height_shift_range=0.1,\n",
        "            channel_shift_range = 30,\n",
        "            horizontal_flip = True,)\n",
        "\n",
        "valid_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "dataset_dir = Path().joinpath(dataset_name)\n",
        "\n",
        "num_classes = 3\n",
        "classes = [\"NO_PERSON\",\n",
        "          \"ALL_THE_PEOPLE\",\n",
        "          \"SOMEONE\"]\n",
        "\n",
        "training_dir = dataset_dir.joinpath(\"training\")\n",
        "train_gen = train_data_gen.flow_from_directory(str(training_dir),\n",
        "                                              batch_size=bs,\n",
        "                                              classes=classes,\n",
        "                                              class_mode=\"categorical\",\n",
        "                                              shuffle=True,\n",
        "                                              target_size=(img_h, img_w),\n",
        "                                              seed=SEED)\n",
        "\n",
        "validation_dir = dataset_dir.joinpath(\"validation\")\n",
        "valid_gen = valid_data_gen.flow_from_directory(str(validation_dir),\n",
        "                                              batch_size=bs,\n",
        "                                              classes=classes,\n",
        "                                              class_mode=\"categorical\",\n",
        "                                              shuffle=True,\n",
        "                                              target_size=(img_h, img_w),\n",
        "                                              seed=SEED)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
        "                                              output_types=(tf.float32, tf.float32),\n",
        "                                              output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
        "                                              output_types=(tf.float32, tf.float32),\n",
        "                                              output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "valid_dataset = valid_dataset.repeat()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5164 images belonging to 3 classes.\n",
            "Found 450 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r53GH4F8jyYR"
      },
      "source": [
        "# Class Weighting\n",
        "In the given datasets classes are not represented with an equal amount of samples, weighting deals with such issue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O96gZ_BJjyYT"
      },
      "source": [
        "from collections import Counter\n",
        "itemCt = Counter(train_gen.classes)\n",
        "maxCt = float(max(itemCt.values()))\n",
        "class_weight = {clsID : maxCt/numImg for clsID, numImg in itemCt.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRdo8h2cjyYZ"
      },
      "source": [
        "## LeNet+: LeNet with 1 convolutional layer more and ReLu\n",
        "LaNet is a simple model originally conceived for handwritten digits classification. The problem at hand is far harder thus a slightly more complex network has been chosen as a starting point. ReLu was not used either in LaNet but as of todays proves to be a very solid choice as an activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QghTKcDFjyYb"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# First Convolution\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(img_h, img_w, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second Convolution\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Third Convolution\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=num_classes, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XLZr81GjyYb"
      },
      "source": [
        "# LeNetb: LeNet+ with Batch Normalization\n",
        "TODO: retrain this with no dropout, may be the answer to validation instability (should still not work incredibly well)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBiNGoQRjyYc"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# First Convolution\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(img_h, img_w, 3)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second Convolution\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Third Convolution\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(units=num_classes, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSSiWNiVjyYd"
      },
      "source": [
        "# Xception Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMMPvs1DjyYf"
      },
      "source": [
        "# Xception richiede input normalizzati in ingresso (procedura già svolta in ImageDataGenerator)\n",
        "base_model = tf.keras.applications.Xception(\n",
        "    weights='imagenet',\n",
        "    input_shape=(img_h, img_w, 3),\n",
        "    include_top=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAKFWDeqjyYj"
      },
      "source": [
        "# tlX: transfer learning with Xception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ogID8objyYk"
      },
      "source": [
        "base_model.trainable = False\n",
        "inputs = tf.keras.Input(shape=(img_h, img_w, 3))\n",
        "x = base_model(inputs, training=False) # training flag previene l'aggiornamento dei layer di Batch Normalization (importante per il fine tuning)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(units=num_classes, activation=\"softmax\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"]) \n",
        "epochs = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2qAR7iMjyYm"
      },
      "source": [
        "# ftX: fine tuning with Xception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-3EB4ETjyYn"
      },
      "source": [
        "if not colab:\n",
        "    model = tf.keras.models.load_model(Path().joinpath(\"Results\", \"tlX.h5\"))\n",
        "else:\n",
        "    model = tf.keras.models.load_model(\"/content/Results/tlX.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZxl_rxzjyYo"
      },
      "source": [
        "# ChennaiNet: improving tlX\n",
        "Using tlX and ftX as a starting point the same training procedure is applied but the pooling is changed, a flattening layer and a dense layer with 0.5 Dropout are added taking inspiration from https://arxiv.org/pdf/2009.08369.pdf ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDGsYA1ljyYp"
      },
      "source": [
        "base_model.trainable = False\n",
        "inputs = tf.keras.Input(shape=(img_h, img_w, 3))\n",
        "x = base_model(inputs, training=False) # training flag previene l'aggiornamento dei layer di Batch Normalization (importante per il fine tuning)\n",
        "x = AveragePooling2D(pool_size=(5, 5))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, kernel_constraint=max_norm(4.))(x)\n",
        "x = Activation(\"relu\")\n",
        "x = Dropout(0.5)\n",
        "x = Dense(units=num_classes, activation=\"softmax\") \n",
        "\n",
        "\n",
        "outputs = Dense(units=num_classes, activation=\"softmax\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"]) \n",
        "epochs = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6uXEQjqjyYq"
      },
      "source": [
        "if not colab:\n",
        "    model = tf.keras.models.load_model(Path().joinpath(\"Results\", \"chennai_net.h5\"))\n",
        "else:\n",
        "    model = tf.keras.models.load_model(\"/content/Results/chennai_net.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WQqbQ82jyYs"
      },
      "source": [
        "# KadapaNet: more parameters and back to global pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQclDFh5jyYt"
      },
      "source": [
        "base_model.trainable = False \n",
        "inputs = tf.keras.Input(shape=(img_h, img_w, 3))\n",
        "x = base_model(inputs, training=False) # training flag previene l'aggiornamento dei layer di Batch Normalization (importante per il fine tuning)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(2048, activation=\"relu\")(x)\n",
        "  \n",
        "outputs = Dense(units=num_classes, activation=\"softmax\")(x)\n",
        "epochs = 40\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer= \"adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcfM1QOEjyYy"
      },
      "source": [
        "if not colab:\n",
        "    model = tf.keras.models.load_model(Path().joinpath(\"Results\", \"kadapa_net.h5\"))\n",
        "else:\n",
        "    model = tf.keras.models.load_model(\"/content/Results/kadapa_net.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_k48w7BjyYz"
      },
      "source": [
        "# LebakaNet: one more dense layer\n",
        "This is the classifier structure used in the Xception paper https://arxiv.org/abs/1610.02357, Dropout layers are not present understand if adding them is needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQVly9adjyYz"
      },
      "source": [
        "base_model.trainable = False \n",
        "inputs = tf.keras.Input(shape=(img_h, img_w, 3))\n",
        "x = base_model(inputs, training=False) # training flag previene l'aggiornamento dei layer di Batch Normalization (importante per il fine tuning)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(2048, activation=\"relu\")(x)\n",
        "# x = Dropout(0)\n",
        "x = Dense(2048, activation=\"relu\")(x)\n",
        "# x = Dropout(0)\n",
        "  \n",
        "outputs = Dense(units=num_classes, activation=\"softmax\")(x)\n",
        "epochs = 40\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer= \"adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJH-cHYYjyY7"
      },
      "source": [
        "# Set re-loaded transfer model to trainable for fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOrtJBlejyY-"
      },
      "source": [
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.python.keras.engine.training.Model):\n",
        "        base_model = layer\n",
        "base_model.trainable = True\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),  # Low learning rate\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXwrVcDOjyZB"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHp0uXeojyZB"
      },
      "source": [
        "callbacks_dir = Path().joinpath(\"Callbacks\")\n",
        "callbacks_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "now = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
        "\n",
        "model_name = \"CNN\"\n",
        "\n",
        "callback_dir = callbacks_dir.joinpath(model_name + '_' + str(now))\n",
        "callback_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "callbacks = []\n",
        "\n",
        "# Model checkpoint\n",
        "ckpt_dir = callback_dir.joinpath(\"ckpts\")\n",
        "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=str(ckpt_dir.joinpath(\"cp.ckpt\")), \n",
        "                                                   save_weights_only=True)\n",
        "callbacks.append(ckpt_callback)\n",
        "\n",
        "# Visualize Learning on Tensorboard\n",
        "tb_dir = callback_dir.joinpath(\"tb_logs\")\n",
        "tb_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=str(tb_dir),\n",
        "                                             profile_batch=0,\n",
        "                                             histogram_freq=1) \n",
        "callbacks.append(tb_callback)\n",
        "\n",
        "# Early Stopping\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    callbacks.append(es_callback)\n",
        "    \n",
        "# Learning Rate Annhealing\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=0, factor=0.5, min_lr=0.00001)\n",
        "lr_annhealing = True\n",
        "\n",
        "if lr_annhealing:\n",
        "    callbacks.append(learning_rate_reduction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MlMuiD5jyZG"
      },
      "source": [
        "# Tensorboard\n",
        "After running the cell below open [this link](http://localhost:6009) to view Tensorboard on a full browser page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj9FJxUBjyZH",
        "outputId": "3bb1e3fe-e4c1-4ff8-8374-5975bae6a666"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./Callbacks --port 6009"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-9a943befa7c9629e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-9a943befa7c9629e\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6009;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLA5CSBXjyZP"
      },
      "source": [
        "# Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RomnmHU4jyZQ",
        "outputId": "9a6950c3-7d8d-4744-e42e-6822e18ca16d"
      },
      "source": [
        "model.fit(x=train_dataset,\n",
        "          epochs=epochs,\n",
        "          steps_per_epoch=len(train_gen),\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=len(valid_gen), \n",
        "          callbacks=callbacks,\n",
        "          class_weight = class_weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "323/323 [==============================] - 242s 748ms/step - loss: 2.6131 - accuracy: 0.4390 - val_loss: 0.9108 - val_accuracy: 0.5533\n",
            "Epoch 2/100\n",
            "323/323 [==============================] - 227s 702ms/step - loss: 0.9672 - accuracy: 0.5062 - val_loss: 0.8301 - val_accuracy: 0.6044\n",
            "Epoch 3/100\n",
            "323/323 [==============================] - 226s 701ms/step - loss: 0.9513 - accuracy: 0.5196 - val_loss: 0.7809 - val_accuracy: 0.6089\n",
            "Epoch 4/100\n",
            "323/323 [==============================] - 226s 701ms/step - loss: 0.9320 - accuracy: 0.5153 - val_loss: 0.8070 - val_accuracy: 0.6111\n",
            "Epoch 5/100\n",
            "323/323 [==============================] - 226s 699ms/step - loss: 0.9284 - accuracy: 0.5337 - val_loss: 0.7654 - val_accuracy: 0.6178\n",
            "Epoch 6/100\n",
            "323/323 [==============================] - 226s 699ms/step - loss: 0.9202 - accuracy: 0.5424 - val_loss: 0.7493 - val_accuracy: 0.6533\n",
            "Epoch 7/100\n",
            "323/323 [==============================] - 226s 700ms/step - loss: 0.9160 - accuracy: 0.5320 - val_loss: 0.8220 - val_accuracy: 0.6578\n",
            "Epoch 8/100\n",
            "323/323 [==============================] - 226s 701ms/step - loss: 0.8971 - accuracy: 0.5552 - val_loss: 0.8245 - val_accuracy: 0.6244\n",
            "Epoch 9/100\n",
            "323/323 [==============================] - 225s 698ms/step - loss: 0.8944 - accuracy: 0.5482 - val_loss: 0.7185 - val_accuracy: 0.6711\n",
            "Epoch 10/100\n",
            "323/323 [==============================] - 226s 700ms/step - loss: 0.8937 - accuracy: 0.5600 - val_loss: 0.7171 - val_accuracy: 0.6756\n",
            "Epoch 11/100\n",
            "323/323 [==============================] - 226s 700ms/step - loss: 0.9033 - accuracy: 0.5533 - val_loss: 0.7691 - val_accuracy: 0.6489\n",
            "Epoch 12/100\n",
            "323/323 [==============================] - 226s 701ms/step - loss: 0.8883 - accuracy: 0.5591 - val_loss: 0.7154 - val_accuracy: 0.6667\n",
            "Epoch 13/100\n",
            "323/323 [==============================] - 226s 700ms/step - loss: 0.8906 - accuracy: 0.5664 - val_loss: 0.7873 - val_accuracy: 0.5467\n",
            "Epoch 14/100\n",
            "323/323 [==============================] - 227s 701ms/step - loss: 0.8988 - accuracy: 0.5542 - val_loss: 0.7460 - val_accuracy: 0.6533\n",
            "Epoch 15/100\n",
            "323/323 [==============================] - 226s 701ms/step - loss: 0.8860 - accuracy: 0.5556 - val_loss: 0.7790 - val_accuracy: 0.6511\n",
            "Epoch 16/100\n",
            "323/323 [==============================] - 226s 701ms/step - loss: 0.8880 - accuracy: 0.5600 - val_loss: 0.9287 - val_accuracy: 0.5711\n",
            "Epoch 17/100\n",
            "323/323 [==============================] - 226s 700ms/step - loss: 0.8867 - accuracy: 0.5517 - val_loss: 0.7371 - val_accuracy: 0.6622\n",
            "Epoch 18/100\n",
            "323/323 [==============================] - 226s 700ms/step - loss: 0.8869 - accuracy: 0.5604 - val_loss: 0.7384 - val_accuracy: 0.6711\n",
            "Epoch 19/100\n",
            "323/323 [==============================] - 226s 701ms/step - loss: 0.8858 - accuracy: 0.5598 - val_loss: 0.7432 - val_accuracy: 0.6756\n",
            "Epoch 20/100\n",
            "323/323 [==============================] - 226s 701ms/step - loss: 0.8904 - accuracy: 0.5496 - val_loss: 0.8740 - val_accuracy: 0.6600\n",
            "Epoch 21/100\n",
            "323/323 [==============================] - 226s 701ms/step - loss: 0.8924 - accuracy: 0.5660 - val_loss: 0.7075 - val_accuracy: 0.6867\n",
            "Epoch 22/100\n",
            "323/323 [==============================] - 226s 700ms/step - loss: 0.8801 - accuracy: 0.5633 - val_loss: 0.6813 - val_accuracy: 0.6644\n",
            "Epoch 23/100\n",
            "323/323 [==============================] - 226s 701ms/step - loss: 0.8727 - accuracy: 0.5749 - val_loss: 0.6904 - val_accuracy: 0.6933\n",
            "Epoch 24/100\n",
            "323/323 [==============================] - 226s 700ms/step - loss: 0.8669 - accuracy: 0.5701 - val_loss: 0.7091 - val_accuracy: 0.6867\n",
            "Epoch 25/100\n",
            " 73/323 [=====>........................] - ETA: 2:41 - loss: 0.8846 - accuracy: 0.5651"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re7DN1uVjyZV"
      },
      "source": [
        "model.save(Path().joinpath(\"Results\", \"model\" + datetime.now().strftime('%b%d_%H-%M-%S')+ \".h5\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t__RPKBvjyZY"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIbagjh3jyZZ"
      },
      "source": [
        "## Test Time Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xSq-0r0jyZZ"
      },
      "source": [
        "tta = False\n",
        "\n",
        "def flip_lr(images):\n",
        "    return np.flip(images, axis=2)\n",
        "\n",
        "def shift(images, shift, axis):\n",
        "    return np.roll(images, shift, axis=axis)\n",
        "\n",
        "def rotate(images, angle):\n",
        "    return sp.ndimage.rotate(\n",
        "        images, angle, axes=(1,2),\n",
        "        reshape=False, mode='nearest')\n",
        "\n",
        "def combine_predictions(predictions):\n",
        "    pred_agg = np.mean(predictions, axis=0)\n",
        "    preds = np.argmax(pred_agg, axis=-1)\n",
        "    return preds\n",
        "\n",
        "def tta_predict(m, x_test):\n",
        "    pred = m.predict(x_test)\n",
        "\n",
        "    pred_f = m.predict(flip_lr(x_test))\n",
        "\n",
        "    pred_w0 = m.predict(shift(x_test, -3, axis=2))\n",
        "    pred_w1 = m.predict(shift(x_test, 3, axis=2))\n",
        "\n",
        "    pred_h0 = m.predict(shift(x_test, -3, axis=1))\n",
        "    pred_h1 = m.predict(shift(x_test, 3, axis=1))\n",
        "\n",
        "    pred_r0 = m.predict(rotate(x_test, -10))\n",
        "    pred_r1 = m.predict(rotate(x_test, 10))\n",
        "    # out = combine_predictions(np.stack((pred, pred_h0, pred_h1, pred_w0, pred_w1, pred_f, pred_r0, pred_r1)))\n",
        "    out = combine_predictions(np.stack((pred, pred_h0, pred_h1, pred_w0, pred_w1, pred_f)))\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arnGh-5pjyZg"
      },
      "source": [
        "results = {}\n",
        "for path in Path().joinpath(dataset_name, \"test\").glob(\"*.jpg\"):\n",
        "    image = Image.open(str(path)).convert(\"RGB\")\n",
        "    image = image.resize((img_w, img_h), Image.ANTIALIAS)\n",
        "    image = np.array(image)\n",
        "    image = np.expand_dims(image, 0)\n",
        "    image = np.float32(image) / 255.0\n",
        "    if not tta:\n",
        "        results[path.name]= model.predict(image).argmax(axis=-1)[0] \n",
        "    else:\n",
        "        results[path.name] = tta_predict(model, image)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7NcOcd6jyZj"
      },
      "source": [
        "csv_fname = \"results_\" + datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "Path().joinpath(\"Results\").mkdir(parents=True, exist_ok=True)\n",
        "with open(Path().joinpath(\"Results\", csv_fname), \"w\") as f:\n",
        "    f.write(\"Id,Category\\n\")\n",
        "    for key, value in results.items():\n",
        "        f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}