{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM2ukdDX4tDY"
      },
      "source": [
        "# VISUAL QUESTION ANSWERING\n",
        "We started from a base model which comprised a convolutional net in parallel with a recurrent Net. The convolutional section dealt with images while the convolutional part delt with questions (natural language).  \n",
        "Approaches:\n",
        "- Base ensemble model: we trained 3 different networks for each question category:\n",
        "  - yes/no\n",
        "  - counting\n",
        "  - other\n",
        "  on top of this we trained a classifier which would recognise the class to which each question belonged. The classifier was a recurrent network with Bidirectional GRU.\n",
        "  Accuracy 0.034 (probably due to an error at the prediction phase)\n",
        "- Transfer Model: we used NasNetMobile (Large) as the convolutional network and left GRUs on the recurrent network. We left the whole transfer model frozen and then we ran another training round with the model unfrozen and a low learning rate (fine tuning)\n",
        "Accuracy 0.363 (not fine tuned)\n",
        "Accuracy 0.365 (fine tuned)\n",
        "- Transfer Model V2: on top of the transfer model we also pre-loaded glove weights on the embedding layer and marked it as non trainable. \n",
        "Accuracy 0.25 (not fine tuned)\n",
        "- Transfer Model V2 (ensemble): we followed the same procedure of the base ensemble model but we used the transfer model v2 as a base, moreover we tweaked for each of the 3 question models regularization in order to better fit the different problems\n",
        "Accuracy 0.12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndo0FOClD285",
        "outputId": "b6191738-6541-4f81-94c7-3663c1951207"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp \"/content/drive/My Drive/VQA_Dataset.zip\" .\n",
        "!mkdir Results\n",
        "!unzip -q VQA_Dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NiGfRU0fg-h",
        "outputId": "11fa9d8d-ca26-4747-8050-89df513ad546"
      },
      "outputs": [],
      "source": [
        "!pip install focal-loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyhbGNkAHGyd"
      },
      "outputs": [],
      "source": [
        "# Cell output set up for Jupyter\n",
        "from pathlib import Path\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQlGKPlKHLUi"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "import random \n",
        "import math\n",
        "\n",
        "SEED = 1234\n",
        "tf.random.set_seed(SEED) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZAy5DBZsFo_"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVVez4_yHVCg"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Dict, Union, Optional, Callable\n",
        "\n",
        "@dataclass\n",
        "class Config: \n",
        "  max_length: int = None\n",
        "  batch_size: int = 64\n",
        "  split: float = 0.8\n",
        "  dataset_name: Path = Path(\"VQA_Dataset\")\n",
        "  augmentation: Dict[str, Union[bool,int,Optional[Callable],str]] = None\n",
        "  # 2 in case of yes_or_no, 6 in case of counting, 50 in case of other, 58 in case of all\n",
        "  num_classes: int = 58\n",
        "  img_w: int = 350\n",
        "  img_h: int = 200\n",
        "  wtoi: int = None\n",
        "  #Questions can be 'all','yes_or_no','counting','other'\n",
        "  questions: str = 'all'\n",
        "  tokeinizer = None\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Umy9jUAoSJuE"
      },
      "outputs": [],
      "source": [
        "labels_dict = {\n",
        "        '0': 0,\n",
        "        '1': 1,\n",
        "        '2': 2,\n",
        "        '3': 3,\n",
        "        '4': 4,\n",
        "        '5': 5,\n",
        "        'apple': 6,\n",
        "        'baseball': 7,\n",
        "        'bench': 8,\n",
        "        'bike': 9,\n",
        "        'bird': 10,\n",
        "        'black': 11,\n",
        "        'blanket': 12,\n",
        "        'blue': 13,\n",
        "        'bone': 14,\n",
        "        'book': 15,\n",
        "        'boy': 16,\n",
        "        'brown': 17,\n",
        "        'cat': 18,\n",
        "        'chair': 19,\n",
        "        'couch': 20,\n",
        "        'dog': 21,\n",
        "        'floor': 22,\n",
        "        'food': 23,\n",
        "        'football': 24,\n",
        "        'girl': 25,\n",
        "        'grass': 26,\n",
        "        'gray': 27,\n",
        "        'green': 28,\n",
        "        'left': 29,\n",
        "        'log': 30,\n",
        "        'man': 31,\n",
        "        'monkey bars': 32,\n",
        "        'no': 33,\n",
        "        'nothing': 34,\n",
        "        'orange': 35,\n",
        "        'pie': 36,\n",
        "        'plant': 37,\n",
        "        'playing': 38,\n",
        "        'red': 39,\n",
        "        'right': 40,\n",
        "        'rug': 41,\n",
        "        'sandbox': 42,\n",
        "        'sitting': 43,\n",
        "        'sleeping': 44,\n",
        "        'soccer': 45,\n",
        "        'squirrel': 46,\n",
        "        'standing': 47,\n",
        "        'stool': 48,\n",
        "        'sunny': 49,\n",
        "        'table': 50,\n",
        "        'tree': 51,\n",
        "        'watermelon': 52,\n",
        "        'white': 53,\n",
        "        'wine': 54,\n",
        "        'woman': 55,\n",
        "        'yellow': 56,\n",
        "        'yes': 57\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFyOXbZayv3R",
        "outputId": "2e34332e-f407-449d-db65-b48d87bd1671"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'0': 0,\n",
              " '1': 1,\n",
              " '2': 2,\n",
              " '3': 3,\n",
              " '4': 4,\n",
              " '5': 5,\n",
              " 'apple': 6,\n",
              " 'baseball': 7,\n",
              " 'bench': 8,\n",
              " 'bike': 9,\n",
              " 'bird': 10,\n",
              " 'black': 11,\n",
              " 'blanket': 12,\n",
              " 'blue': 13,\n",
              " 'bone': 14,\n",
              " 'book': 15,\n",
              " 'boy': 16,\n",
              " 'brown': 17,\n",
              " 'cat': 18,\n",
              " 'chair': 19,\n",
              " 'couch': 20,\n",
              " 'dog': 21,\n",
              " 'floor': 22,\n",
              " 'food': 23,\n",
              " 'football': 24,\n",
              " 'girl': 25,\n",
              " 'grass': 26,\n",
              " 'gray': 27,\n",
              " 'green': 28,\n",
              " 'left': 29,\n",
              " 'log': 30,\n",
              " 'man': 31,\n",
              " 'monkey bars': 32,\n",
              " 'no': 33,\n",
              " 'nothing': 34,\n",
              " 'orange': 35,\n",
              " 'pie': 36,\n",
              " 'plant': 37,\n",
              " 'playing': 38,\n",
              " 'red': 39,\n",
              " 'right': 40,\n",
              " 'rug': 41,\n",
              " 'sandbox': 42,\n",
              " 'sitting': 43,\n",
              " 'sleeping': 44,\n",
              " 'soccer': 45,\n",
              " 'squirrel': 46,\n",
              " 'standing': 47,\n",
              " 'stool': 48,\n",
              " 'sunny': 49,\n",
              " 'table': 50,\n",
              " 'tree': 51,\n",
              " 'watermelon': 52,\n",
              " 'white': 53,\n",
              " 'wine': 54,\n",
              " 'woman': 55,\n",
              " 'yellow': 56,\n",
              " 'yes': 57}"
            ]
          },
          "execution_count": 7,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if config.questions == 'yes_or_no':\n",
        "  for key in set(labels_dict.keys()).difference({'yes','no'}):\n",
        "    del labels_dict[key]\n",
        "elif config.questions == 'counting':\n",
        "  for key in set(labels_dict.keys()).difference({'0','1','2','3','4','5'}):\n",
        "    del labels_dict[key]\n",
        "elif config.questions != 'all':\n",
        "  for key in {'yes','no','0','1','2','3','4','5'}:\n",
        "    del labels_dict[key]\n",
        "counter = 0\n",
        "for k in labels_dict.keys():\n",
        "  labels_dict[k] = counter\n",
        "  counter += 1 \n",
        "labels_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgM71MuhsJ1L"
      },
      "source": [
        "# Dataset Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyG0SyGfgrHu"
      },
      "outputs": [],
      "source": [
        "def question_split():\n",
        "  annotations = json.load(config.dataset_name.joinpath(\"train_questions_annotations.json\").open())\n",
        "  yes_or_no_annotations = {}\n",
        "  counting_annotations = {}\n",
        "  other_annotations = {}\n",
        "  class_occurrences = {}\n",
        "  for label in labels_dict:\n",
        "    class_occurrences[label] = 0\n",
        "  \n",
        "  for k,v in annotations.items():\n",
        "    class_occurrences[v['answer']] = class_occurrences[v['answer']] + 1 \n",
        "    if v['answer'] in {'yes','no'}:\n",
        "      dict_to_write = yes_or_no_annotations\n",
        "    elif v['answer'] in {'0','1','2','3','4','5'}:\n",
        "      dict_to_write = counting_annotations\n",
        "    else:\n",
        "      dict_to_write = other_annotations\n",
        "    dict_to_write[k] = v\n",
        "  json.dump(yes_or_no_annotations,config.dataset_name.joinpath(\"yes_or_no.json\").open(\"w+\"))\n",
        "  json.dump(counting_annotations,config.dataset_name.joinpath(\"counting.json\").open(\"w+\"))\n",
        "  json.dump(other_annotations,config.dataset_name.joinpath(\"other.json\").open(\"w+\"))\n",
        "  json.dump(class_occurrences,config.dataset_name.joinpath(\"class_occurrences.json\").open(\"w+\"))\n",
        "question_split()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIRp0IkIIFB8"
      },
      "outputs": [],
      "source": [
        "\n",
        "def split(config : Config,to_split : str): \n",
        "  \n",
        "  train_validation_dict = json.load(config.dataset_name.joinpath(to_split).open())\n",
        "  train_validation_key_set = set(train_validation_dict.keys())\n",
        "  question_num = len(train_validation_key_set)\n",
        "  question_num_train = math.floor(config.split * question_num)\n",
        "  question_num_val = question_num - question_num_train\n",
        "  validation_dict = {}\n",
        "  for i in range(0,question_num_val):\n",
        "    to_add_index = random.randint(0,question_num - i -1)\n",
        "    to_add_value = list(train_validation_dict.values())[to_add_index]\n",
        "    to_add_key = list(train_validation_dict.keys())[to_add_index]\n",
        "    validation_dict[to_add_key] = to_add_value\n",
        "    train_validation_dict.pop(to_add_key)\n",
        "\n",
        "  train_dict = train_validation_dict\n",
        "  json.dump(train_dict,config.dataset_name.joinpath(\"train.json\").open(\"w+\"))\n",
        "  json.dump(validation_dict,config.dataset_name.joinpath(\"validation.json\").open(\"w+\"))\n",
        "\n",
        "path = \"train_questions_annotations.json\" if config.questions == 'all' else \"yes_or_no.json\" if config.questions == 'yes_or_no' else \"counting.json\" if config.questions == 'counting' else 'other.json'\n",
        "split(config,path)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBijykeXsRzu"
      },
      "source": [
        "# VQA Custom Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2C02kxjSW_Fo"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import string\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from collections import OrderedDict\n",
        "\n",
        "def encode_input(data_dict):\n",
        "  question_list = []\n",
        "  question_max = 0\n",
        "  for v in data_dict.values():\n",
        "    question = v['question'].strip()\n",
        "    question_words = len(question.translate(str.maketrans(\"\",\"\",string.punctuation)).split(' '))\n",
        "    question_max = question_words if question_words > question_max else question_max\n",
        "    question_list.append(question)\n",
        "    #question_list_eos.append(question + '<eos>')\n",
        "    #question_list_sos.append('<sos>' + question)\n",
        "    # Create Tokenizer to convert words to integers\n",
        "  tokenizer = Tokenizer(num_words= question_max)\n",
        "  config.tokenizer = tokenizer\n",
        "  tokenizer.fit_on_texts(question_list)\n",
        "  tokenized = tokenizer.texts_to_sequences(question_list)\n",
        "  config.wtoi = len(tokenizer.word_index)\n",
        "  config.max_length = 14\n",
        "  return pad_sequences(tokenized,maxlen=config.max_length)\n",
        "\n",
        "class CustomDataset(tf.keras.utils.Sequence):\n",
        "\n",
        "  def __init__(self,config : Config, which_subset : str, image_generator=None):\n",
        "    self.config = config\n",
        "    self.image_generator = image_generator\n",
        "    self.which_subset = which_subset\n",
        "    if self.which_subset == 'training':\n",
        "      self.data_dict = json.load(Path(config.dataset_name).joinpath(\"train.json\").open(),object_pairs_hook=OrderedDict)\n",
        "    elif self.which_subset == 'validation':\n",
        "      self.data_dict = json.load(Path(config.dataset_name).joinpath(\"validation.json\").open(),object_pairs_hook=OrderedDict)\n",
        "    else:\n",
        "      raise Exception(\"Unsupported which subset: \"+ str(which_subset))\n",
        "    #question_list_eos = []\n",
        "    #question_list_sos = []\n",
        "    self.encoder_inputs = encode_input(self.data_dict)\n",
        "  def __len__(self):\n",
        "    return len(self.data_dict)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item_key = list(self.data_dict.keys())[index]\n",
        "    item_value = self.data_dict[item_key]\n",
        "    item_image_id = item_value['image_id']\n",
        "    item_image = Image.open(Path(self.config.dataset_name).joinpath(\"Images\",item_image_id +  \".png\"))\n",
        "    item_image = item_image.convert(\"RGB\")\n",
        "    item_image = item_image.resize((config.img_w,config.img_h))\n",
        "    item_image_arr = np.array(item_image).transpose((1, 0, 2))\n",
        "    if self.which_subset == 'training' and self.image_generator is not None:\n",
        "      transform = self.image_generator.get_random_transform(item_image_arr.shape, seed=SEED)\n",
        "      item_image_arr = self.image_generator.apply_transform(item_image_arr, transform)\n",
        "    target = int(labels_dict[item_value['answer']])\n",
        "    return {'image':item_image_arr , 'question':self.encoder_inputs[index]} , target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "lfUAFvIvdEOW",
        "outputId": "0fdc3cb8-0972-44f8-8774-836bfc6c5b6b"
      },
      "outputs": [],
      "source": [
        "config.augmentation = {\n",
        "  'rotation_range':20,\n",
        "  'width_shift_range':0.1,\n",
        "  'height_shift_range':0.1,\n",
        "  'zoom_range':0.1,\n",
        "  'horizontal_flip':True,\n",
        "  'fill_mode':\"nearest\",\n",
        "  'rescale':1./255,\n",
        "  'preprocessing_function': None\n",
        "}\n",
        "\n",
        "image_generator = ImageDataGenerator(**config.augmentation)\n",
        "\n",
        "train_dataset = CustomDataset(config,\"training\",image_generator=image_generator)\n",
        "val_dataset = CustomDataset(config,\"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyLnEhhkeEWD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "train_gen = tf.data.Dataset.from_generator(lambda: train_dataset,\n",
        "                                               output_signature=({'image':tf.TensorSpec(shape=(config.img_w, config.img_h, 3),dtype=np.uint8),\n",
        "                                                                 'question':tf.TensorSpec(shape=(config.max_length),dtype=np.int32)},\n",
        "                                                                 tf.TensorSpec(shape=(),dtype=np.int32)\n",
        "                                                                 ))\n",
        "\n",
        "train_gen = train_gen.batch(config.batch_size)\n",
        "\n",
        "train_gen = train_gen.repeat()\n",
        "\n",
        "valid_gen = tf.data.Dataset.from_generator(lambda: val_dataset,\n",
        "                                              output_signature=({'image':tf.TensorSpec(shape=(config.img_w, config.img_h, 3),dtype=np.uint8),\n",
        "                                                                 'question':tf.TensorSpec(shape=(config.max_length),dtype=np.int32)},\n",
        "                                                                 tf.TensorSpec(shape=(),dtype=np.int32)\n",
        "                                                                 ))\n",
        "valid_gen = valid_gen.batch(config.batch_size)\n",
        "\n",
        "valid_gen = valid_gen.repeat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNKb_-pCAKRP"
      },
      "source": [
        "# Pre-Trained Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjTXrTCNB4mL",
        "outputId": "f9605ede-2d56-4d8e-b426-b502e3dec42c"
      },
      "outputs": [],
      "source": [
        "!wget https://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6OCDfTzGStT",
        "outputId": "ef741aed-d5d0-49b7-a9a5-7222ab67e104"
      },
      "outputs": [],
      "source": [
        "!unzip glove.840B.300d.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_1tt7MnANJo"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "embeddings_index = dict()\n",
        "f = Path('./glove.840B.300d.txt').open()\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "    except Exception:\n",
        "      continue\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "f.close()\n",
        "\n",
        "size_of_vocabulary = config.wtoi+1\n",
        "embedding_matrix = np.zeros((size_of_vocabulary, 300))\n",
        "\n",
        "for word, i in config.tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVJDV1oXsaMx"
      },
      "source": [
        "# VQA Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kURrKKWWG_9K",
        "outputId": "d64c9b4d-ecc4-4af7-d283-3b1c0d5a5584"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "question (InputLayer)           [(None, 14)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 14, 256)      608768      question[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "image (InputLayer)              [(None, 350, 200, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 256)          525312      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "sequential_1 (Sequential)       (None, 256)          1738944     image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 512)          0           lstm_1[0][0]                     \n",
            "                                                                 sequential_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 58)           29696       concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 2,902,720\n",
            "Trainable params: 2,900,416\n",
            "Non-trainable params: 2,304\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Import Keras \n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D , Input, LSTM, Embedding, Dense, Activation, BatchNormalization\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "regularizer = tf.keras.regularizers.l2(1e-4)\n",
        "constraint = MaxNorm(3)\n",
        "# Define CNN for Image Input\n",
        "vision_model = Sequential()\n",
        "vision_model.add(Conv2D(64, (3, 3), padding='same', input_shape=(config.img_w, config.img_h, 3),kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint))\n",
        "vision_model.add(BatchNormalization())\n",
        "vision_model.add(Activation('relu'))\n",
        "vision_model.add(Conv2D(64, (3, 3),kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint))\n",
        "vision_model.add(BatchNormalization())\n",
        "vision_model.add(Activation('relu'))\n",
        "vision_model.add(MaxPooling2D((2, 2)))\n",
        "vision_model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint))\n",
        "vision_model.add(BatchNormalization())\n",
        "vision_model.add(Activation('relu'))\n",
        "vision_model.add(Conv2D(128, (3, 3),kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint))\n",
        "vision_model.add(BatchNormalization())\n",
        "vision_model.add(Activation('relu'))\n",
        "vision_model.add(MaxPooling2D((2, 2)))\n",
        "vision_model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint))\n",
        "vision_model.add(BatchNormalization())\n",
        "vision_model.add(Activation('relu'))\n",
        "vision_model.add(Conv2D(256, (3, 3),kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint))\n",
        "vision_model.add(BatchNormalization())\n",
        "vision_model.add(Activation('relu'))\n",
        "vision_model.add(Conv2D(256, (3, 3),kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint))\n",
        "vision_model.add(BatchNormalization())\n",
        "vision_model.add(Activation('relu'))\n",
        "#vision_model.add(MaxPooling2D((2, 2)))\n",
        "#vision_model.add(Flatten())\n",
        "vision_model.add(GlobalAveragePooling2D())\n",
        "\n",
        "image_input = Input(shape=(config.img_w, config.img_h, 3),name='image')\n",
        "encoded_image = vision_model(image_input)\n",
        "\n",
        "# Define RNN for language input\n",
        "question_input = Input(shape=(config.max_length), dtype='int32',name='question')\n",
        "embedded_question = Embedding(input_dim=config.wtoi + 1, output_dim=256)(question_input)\n",
        "encoded_question = LSTM(256,kernel_constraint=constraint,recurrent_constraint=constraint)(embedded_question)\n",
        "\n",
        "# Combine CNN and RNN to create the final model\n",
        "merged = tf.keras.layers.concatenate([encoded_question, encoded_image])\n",
        "output = Dense(config.num_classes, activation='softmax',kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint)(merged)\n",
        "model = Model(inputs=[image_input, question_input], outputs=output)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1mJoyg6toGn"
      },
      "source": [
        "# Visual Transfer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "5VXTntBOtuLD",
        "outputId": "67d25f01-4666-478c-acf8-8082d2792ff9"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D , Input, LSTM, Embedding, Dense, Activation, BatchNormalization\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "regularizer = tf.keras.regularizers.l2(1e-4)\n",
        "constraint = MaxNorm(3)\n",
        "# Define CNN for Image Input\n",
        "vision_model = tf.keras.applications.MobileNetV3Large(include_top=False, input_shape=(config.img_w, config.img_h, 3), pooling=\"avg\")\n",
        "for layer in vision_model.layers:\n",
        "  layer.trainable = False\n",
        "for i in range(-6, 0):\n",
        "  vision_model.layers[i].trainable = True\n",
        "image_input = Input(shape=(config.img_w, config.img_h, 3),name='image')\n",
        "encoded_image = vision_model(image_input)\n",
        "\n",
        "# Define RNN for language input\n",
        "question_input = Input(shape=(config.max_length), dtype='int32',name='question')\n",
        "embedded_question = Embedding(input_dim=config.wtoi + 1, output_dim=256)(question_input)\n",
        "encoded_question = LSTM(256,kernel_constraint=constraint,recurrent_constraint=constraint)(embedded_question)\n",
        "\n",
        "# Combine CNN and RNN to create the final model\n",
        "merged = tf.keras.layers.concatenate([encoded_question, encoded_image])\n",
        "output = Dense(config.num_classes, activation='softmax',kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint)(merged)\n",
        "model = Model(inputs=[image_input, question_input], outputs=output)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_cyC88HC1bD"
      },
      "source": [
        "# Visual and Text Transfer Learning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xRXpko_C_bT",
        "outputId": "5d7a1211-49ba-456a-aa81-a6dc0232f30d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "image (InputLayer)              [(None, 350, 200, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "MobilenetV3large (Functional)   (None, 11, 7, 1280)  4226432     image[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "question (InputLayer)           [(None, 14)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_7 (Glo (None, 1280)         0           MobilenetV3large[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_7 (GlobalM (None, 1280)         0           MobilenetV3large[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten_7 (Flatten)             (None, 98560)        0           MobilenetV3large[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 14, 300)      710100      question[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 101120)       0           global_average_pooling2d_7[0][0] \n",
            "                                                                 global_max_pooling2d_7[0][0]     \n",
            "                                                                 flatten_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_5 (Bidirectional) (None, 512)          857088      embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 101120)       0           concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 101632)       0           bidirectional_5[0][0]            \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 16)           1626112     concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16)           64          dense_15[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 16)           0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 256)          4096        activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 256)          1024        dense_16[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 256)          0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 58)           14848       activation_11[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 7,439,764\n",
            "Trainable params: 2,502,688\n",
            "Non-trainable params: 4,937,076\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Vision Model comes from: https://towardsdatascience.com/metastasis-detection-using-cnns-transfer-learning-and-data-augmentation-684761347b59\n",
        "from tensorflow.keras.layers import GRU, Dropout, Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D , GlobalMaxPooling2D, Input, LSTM, Embedding, Dense, Activation, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "regularizer = None\n",
        "constraint = None\n",
        "# Define CNN for Image Input\n",
        "vision_model = tf.keras.applications.MobileNetV3Large(include_top=False, input_shape=(config.img_w, config.img_h, 3))\n",
        "for layer in vision_model.layers:\n",
        "  layer.trainable = False\n",
        "#for i in range(-6, 0):\n",
        "#  vision_model.layers[i].trainable = True\n",
        "\n",
        "image_input = Input(shape=(config.img_w, config.img_h, 3),name='image')\n",
        "encoded_image = vision_model(image_input)\n",
        "pool_1 = GlobalAveragePooling2D()(encoded_image)\n",
        "pool_2 = GlobalMaxPooling2D()(encoded_image)\n",
        "#flatten = Flatten()(encoded_image)\n",
        "concat = tf.keras.layers.concatenate([pool_1, pool_2])\n",
        "encoded_image = Dropout(0.3)(concat)\n",
        "# Define RNN for language input\n",
        "question_input = Input(shape=(config.max_length), dtype='int32',name='question')\n",
        "embedded_question = Embedding(input_dim=config.wtoi + 1, weights=[embedding_matrix], output_dim=300, trainable=False)(question_input)\n",
        "encoded_question = Bidirectional(GRU(256,\n",
        "                                     kernel_constraint=constraint,\n",
        "                                     recurrent_constraint=constraint,\n",
        "                                     bias_constraint=constraint, \n",
        "                                     kernel_regularizer=regularizer, \n",
        "                                     activity_regularizer=regularizer,\n",
        "                                     recurrent_regularizer=regularizer,\n",
        "                                     ))(embedded_question)\n",
        "\n",
        "# Combine CNN and RNN to create the final model\n",
        "merged = tf.keras.layers.concatenate([encoded_question, encoded_image])\n",
        "x = Dense(256, kernel_regularizer=regularizer, kernel_constraint=constraint)(merged)\n",
        "x = BatchNormalization()(x)\n",
        "x = Activation(\"relu\")(x)\n",
        "output = Dense(config.num_classes, activation='softmax',kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint)(x)\n",
        "model = Model(inputs=[image_input, question_input], outputs=output)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJA4IavpsvAN"
      },
      "source": [
        "# Questions Classification Custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoR1DUgOs7fy"
      },
      "outputs": [],
      "source": [
        "#yes_or_no is class 0, counting is class 1, other is class 2\n",
        "\n",
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "def create_class_questions_dict():\n",
        "  train_validation_dict = json.load(config.dataset_name.joinpath(\"train_questions_annotations.json\").open())\n",
        "  class_questions_dict = defaultdict(dict)\n",
        "  counter = 0\n",
        "  for v in train_validation_dict.values():\n",
        "    if v['answer'] in {'yes','no'}:\n",
        "      class_questions_dict[counter][\"question\"] = v['question']\n",
        "      class_questions_dict[counter][\"target\"] = 0\n",
        "    elif v['answer'] in {'0','1','2','3','4','5'}:\n",
        "      class_questions_dict[counter][\"question\"] = v['question']\n",
        "      class_questions_dict[counter][\"target\"] = 1\n",
        "    else:\n",
        "      class_questions_dict[counter][\"question\"] = v['question']\n",
        "      class_questions_dict[counter][\"target\"] = 2\n",
        "    counter += 1\n",
        "\n",
        "  return class_questions_dict\n",
        "\n",
        "#We need to split it and then to take from validation and training\n",
        "\n",
        "def split_class_questions(config : Config):\n",
        "  questions = create_class_questions_dict()\n",
        "  keys = list(questions.keys())\n",
        "  random.shuffle(keys)\n",
        "  training_samples = int(len(keys) * config.split)\n",
        "  training_keys = keys[0:training_samples]\n",
        "  validation_keys = keys[training_samples:]\n",
        "  training_dict = {}\n",
        "  validation_dict = {}\n",
        "  for k in training_keys:\n",
        "    training_dict[k] = questions[k]\n",
        "\n",
        "  for k in validation_keys:\n",
        "    validation_dict[k] = questions[k]\n",
        "  \n",
        "  return training_dict,validation_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihn1slJjs3-H"
      },
      "outputs": [],
      "source": [
        "class CustomDatasetQuestions(tf.keras.utils.Sequence):\n",
        "  def __init__(self,config : Config,which_subset : str):\n",
        "    self.config = config\n",
        "    training_dict, validation_dict = split_class_questions(config)\n",
        "    self.data_dict = training_dict if which_subset == 'training' else validation_dict\n",
        "    self.encoder_inputs = encode_input(self.data_dict)\n",
        "    print(list(self.data_dict.items())[0:20])\n",
        "  def __len__(self):\n",
        "    return len(self.data_dict)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    item_key = list(self.data_dict.keys())[index]\n",
        "    target = self.data_dict[item_key][\"target\"]\n",
        "    return self.encoder_inputs[index] , target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzk9YQu4tCMw",
        "outputId": "84c41b29-4a4f-4dd1-888e-c1dc8ce71a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(24453, {'question': 'Is the man scared?', 'target': 0}), (56254, {'question': 'What kind of ball is this?', 'target': 2}), (8605, {'question': 'Are all the plants the same height?', 'target': 0}), (13510, {'question': 'Is it sunny?', 'target': 0}), (27204, {'question': 'Is it possible that the man is in danger?', 'target': 0}), (50584, {'question': 'What color is the couch?', 'target': 2}), (39959, {'question': 'What is the weather outside?', 'target': 2}), (19790, {'question': 'Is the woman feeling lonely?', 'target': 0}), (17273, {'question': 'Is the girl falling?', 'target': 0}), (26327, {'question': 'What colors are in the area rug?', 'target': 2}), (36199, {'question': 'What is the old man holding?', 'target': 2}), (57715, {'question': 'What is next to the dog?', 'target': 2}), (24570, {'question': 'Is the woman alone?', 'target': 0}), (51807, {'question': 'Is there a person on the table top?', 'target': 0}), (48578, {'question': 'Are all of the individuals featured in this picture male?', 'target': 0}), (53861, {'question': 'What is the bicycle standing against?', 'target': 2}), (30406, {'question': 'Is the old lady looking at the cat?', 'target': 0}), (52964, {'question': \"Does the man's clothing match the couch?\", 'target': 0}), (4105, {'question': \"What color is the baby's shirt?\", 'target': 2}), (29085, {'question': 'What type of fruit is on the table?', 'target': 2})]\n",
            "[(52947, {'question': 'What is this girl thinking?', 'target': 2}), (32441, {'question': 'How many pillows on the couch?', 'target': 1}), (58656, {'question': 'Does the boy wearing socks?', 'target': 0}), (17102, {'question': 'How many children are in the park?', 'target': 1}), (43375, {'question': 'Is there someone hanging on the monkey bars?', 'target': 0}), (57755, {'question': 'Are both ladies sitting?', 'target': 0}), (39339, {'question': 'What color is his shirt?', 'target': 2}), (22722, {'question': 'What is the person holding?', 'target': 2}), (31595, {'question': 'Is the boy taking the bread from the owl?', 'target': 0}), (27253, {'question': 'How many plants are there?', 'target': 1}), (38272, {'question': 'Does the man look scared?', 'target': 0}), (28873, {'question': \"Is the dog's tail reminiscent of the cloud?\", 'target': 0}), (49228, {'question': 'How many pots on top of the fireplace?', 'target': 1}), (1543, {'question': 'How many hamburgers are on the blanket?', 'target': 1}), (1289, {'question': 'What color is the wine?', 'target': 2}), (56082, {'question': 'Are there curtains on the window?', 'target': 0}), (56259, {'question': \"What is the girl's hand?\", 'target': 2}), (31576, {'question': 'Does a child live in this house?', 'target': 0}), (3124, {'question': 'Where is she taking the food?', 'target': 2}), (43098, {'question': 'How many birds are afraid of the dog?', 'target': 1})]\n"
          ]
        }
      ],
      "source": [
        "train_dataset = CustomDatasetQuestions(config,\"training\")\n",
        "val_dataset = CustomDatasetQuestions(config,\"validation\")\n",
        "\n",
        "import numpy as np\n",
        "train_gen = tf.data.Dataset.from_generator(lambda: train_dataset,\n",
        "                                               output_signature=(\n",
        "                                                                 tf.TensorSpec(shape=(config.max_length),dtype=np.int32),\n",
        "                                                                 tf.TensorSpec(shape=(),dtype=np.int32)\n",
        "                                                                 ))\n",
        "\n",
        "train_gen = train_gen.batch(config.batch_size)\n",
        "\n",
        "train_gen = train_gen.repeat()\n",
        "\n",
        "valid_gen = tf.data.Dataset.from_generator(lambda: val_dataset,\n",
        "                                               output_signature=(\n",
        "                                                                 tf.TensorSpec(shape=(config.max_length),dtype=np.int32),\n",
        "                                                                 tf.TensorSpec(shape=(),dtype=np.int32)\n",
        "                                                                 ))\n",
        "\n",
        "valid_gen = valid_gen.batch(config.batch_size)\n",
        "\n",
        "valid_gen = valid_gen.repeat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyitCZGrtEgT"
      },
      "source": [
        "# Question Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo6KCVhntIX_",
        "outputId": "41beda16-3b24-4944-b26a-f8506e926a00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "question (InputLayer)        [(None, 14)]              0         \n",
            "_________________________________________________________________\n",
            "embedding_7 (Embedding)      (None, 14, 256)           627456    \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 32)                26304     \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 3)                 96        \n",
            "=================================================================\n",
            "Total params: 653,856\n",
            "Trainable params: 653,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, GlobalAveragePooling2D , Input, LSTM, Embedding, Dense, Activation, BatchNormalization, Bidirectional,GRU\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "\n",
        "constraint = MaxNorm(3)\n",
        "regularizer = tf.keras.regularizers.l1_l2(1e-2)\n",
        "question_input = Input(shape=(config.max_length), dtype='int32',name='question')\n",
        "embedded_question = Embedding(input_dim=config.wtoi + 1, output_dim=256)(question_input)\n",
        "encoded_question = Bidirectional(GRU(16,kernel_constraint=constraint,recurrent_constraint=constraint, bias_constraint=constraint, bias_regularizer=regularizer, recurrent_regularizer=regularizer, kernel_regularizer=regularizer, activity_regularizer=regularizer))(embedded_question)\n",
        "output = Dense(3, activation='softmax',kernel_regularizer=regularizer,use_bias=False,kernel_constraint=constraint)(encoded_question)\n",
        "model = Model(inputs=question_input, outputs=output)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBNqH10csgGW"
      },
      "source": [
        "# Compilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rG5n9rsQixFc"
      },
      "outputs": [],
      "source": [
        "from focal_loss import SparseCategoricalFocalLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "WIPGWnQ3J0dQ",
        "outputId": "d3a391bf-c020-4007-f976-7000df2cc1e7"
      },
      "outputs": [],
      "source": [
        "from focal_loss import SparseCategoricalFocalLoss\n",
        "# Optimization params\n",
        "# -------------------\n",
        "\n",
        "# Loss\n",
        "loss = SparseCategoricalFocalLoss(gamma=2)\n",
        "\n",
        "# learning rate\n",
        "lr = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "# -------------------\n",
        "\n",
        "# Validation metrics\n",
        "# ------------------\n",
        "\n",
        "metrics = ['sparse_categorical_accuracy']\n",
        "# ------------------\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH0iJMnssjCp"
      },
      "source": [
        "# Callback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fmfDGuDK6qy"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau \n",
        "import os \n",
        "\n",
        "exps_dir = Path(\"/\").joinpath(\"content\", \"drive\", \"MyDrive\", \"Colab Notebooks\", \"Homework3\",\"Results\")\n",
        "exps_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "now = datetime.now().strftime('%b%d_%H-%M-%S')\n",
        "\n",
        "model_name = 'NaiveModel_' + config.questions\n",
        "\n",
        "exp_dir = Path(exps_dir).joinpath(model_name + '_' + str(now))\n",
        "exp_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "callbacks = []\n",
        "\n",
        "# Model checkpoint\n",
        "ckpt_dir = Path(exp_dir).joinpath('ckpts')\n",
        "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n",
        "                                                   save_weights_only=True)  # False to save the model directly\n",
        "callbacks.append(ckpt_callback)\n",
        "\n",
        "# Visualize Learning on Tensorboard\n",
        "# ---------------------------------\n",
        "tb_dir = Path(exp_dir).joinpath('tb_logs')\n",
        "tb_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "# By default shows losses and metrics for both training and validation\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n",
        "                                             profile_batch=0,\n",
        "                                             histogram_freq=0)  # if 1 shows weights histograms\n",
        "callbacks.append(tb_callback)\n",
        "\n",
        "# Early Stopping\n",
        "# --------------\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    callbacks.append(es_callback)\n",
        "\n",
        "# Learning Rate Annhealing\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_sparse_categorical_accuracy', patience=3, verbose=1, factor=0.5, min_lr=1e-6)\n",
        "\n",
        "lr_scheduling = True\n",
        "if lr_scheduling:\n",
        "    callbacks.append(learning_rate_reduction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pl8uFqu1sk99"
      },
      "source": [
        "# Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-sj_H5oSKdXz",
        "outputId": "d11967ea-ecf3-47fc-eb9a-ca9ca09b5a33"
      },
      "outputs": [],
      "source": [
        "\n",
        "steps_train = len(train_dataset) // config.batch_size \n",
        "steps_val = len(val_dataset) // config.batch_size \n",
        "model.fit(x=train_gen,\n",
        "          epochs=100,  #### set repeat in training dataset\n",
        "          steps_per_epoch=steps_train,\n",
        "          validation_data=valid_gen,\n",
        "          validation_steps=steps_val, \n",
        "          callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqvXMqbqsouR"
      },
      "source": [
        "# Model prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1mL3TgcuybW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def create_csv(results, results_dir='./'):\n",
        "\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')\n",
        "\n",
        "def prediction(model):\n",
        "  test_questions = json.load(config.dataset_name.joinpath(\"test_questions.json\").open())\n",
        "  questions = encode_input(test_questions)\n",
        "  counter = 0\n",
        "  results = {}\n",
        "  for k,v in test_questions.items():\n",
        "    image = Image.open(Path(config.dataset_name).joinpath(\"Images\",v[\"image_id\"] + \".png\"))\n",
        "    item_image = image.convert(\"RGB\")\n",
        "    item_image = item_image.resize((config.img_w,config.img_h))\n",
        "    item_image_arr = np.array(item_image).transpose((1, 0, 2))\n",
        "    item_image_arr = np.float32(item_image_arr) / 255.0\n",
        "    item_image_arr = tf.expand_dims(item_image_arr,axis=0)\n",
        "    input_dict = {'image':item_image_arr , 'question':tf.expand_dims(questions[counter],axis=0)}\n",
        "    results[k] = model.predict(input_dict).argmax(axis=-1)[0]\n",
        "    counter +=1\n",
        "  create_csv(results)\n",
        "\n",
        "def prediction_ensemble(config,yes_or_no_model,counting_model,other_model,classifier_model):\n",
        "  test_questions = json.load(config.dataset_name.joinpath(\"test_questions.json\").open())\n",
        "  questions = encode_input(test_questions)\n",
        "  counter = 0\n",
        "  results = {}\n",
        "  for k,v in test_questions.items():\n",
        "    image = Image.open(Path(config.dataset_name).joinpath(\"Images\",v[\"image_id\"]+\".png\"))\n",
        "    item_image = image.convert(\"RGB\")\n",
        "    item_image = item_image.resize((config.img_w,config.img_h))\n",
        "    item_image_arr = np.array(item_image).transpose((1, 0, 2))\n",
        "    item_image_arr = np.float32(item_image_arr) / 255.0\n",
        "    question_type = classifier_model.predict(questions[counter]).argmax(axis=-1)[0]\n",
        "    input_dict = {'image':tf.expand_dims(item_image_arr,axis=0) , 'question':tf.expand_dims(questions[counter],axis=0)}\n",
        "    if question_type == 0:\n",
        "      results[k] = yes_or_no_model.predict(input_dict).argmax(axis=-1)[0]\n",
        "    elif question_type == 1:\n",
        "      results[k] = counting_model.predict(input_dict).argmax(axis=-1)[0]\n",
        "    else:\n",
        "      results[k] = other_model.predict(input_dict).argmax(axis=-1)[0]\n",
        "    counter +=1\n",
        "  create_csv(results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bCDXUfaiavy",
        "outputId": "72e60998-d81c-47fe-dedc-5fbcf98199af"
      },
      "outputs": [],
      "source": [
        "prediction_ensemble(config,yes_or_no_model,model_counting,model_other,question_classifier)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "jM2ukdDX4tDY",
        "vZAy5DBZsFo_",
        "pgM71MuhsJ1L",
        "wBijykeXsRzu",
        "SNKb_-pCAKRP",
        "nVJDV1oXsaMx",
        "Q1mJoyg6toGn",
        "M_cyC88HC1bD",
        "jJA4IavpsvAN",
        "GyitCZGrtEgT",
        "TBNqH10csgGW",
        "BH0iJMnssjCp",
        "Pl8uFqu1sk99",
        "tqvXMqbqsouR"
      ],
      "machine_shape": "hm",
      "name": "homework3.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
