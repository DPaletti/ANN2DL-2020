{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "Copy of homework1_late_night_edition.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GefsMXhCjyXP"
      },
      "source": [
        "#  Setup\n",
        "1. Jupyter Environment Setup\n",
        "2. Dataset Split in ImageDataGenerator format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbfmlVWsjyXX"
      },
      "source": [
        "colab = True # set this to true and activate next cell"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USXk3TcBjyXr",
        "outputId": "b02284f6-90a6-4b43-d2bf-e3e940824a61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp \"/content/drive/My Drive/artificial-neural-networks-and-deep-learning-2020.zip\" .\n",
        "!mkdir Results\n",
        "!cp \"/content/drive/My Drive/tlX.h5\" ./Results\n",
        "!unzip -q artificial-neural-networks-and-deep-learning-2020.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPrwVCPrjyXt"
      },
      "source": [
        "# Cell output set up for Jupyter\n",
        "from pathlib import Path\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8mifj-KjyX3"
      },
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "SEED = 1234\n",
        "random.seed(SEED)\n",
        "\n",
        "test_pictures_n = 1684 # 70/30 ratio for training and validation\n",
        "target_file_name = \"train_gt.json\"\n",
        "dataset_name = \"MaskDataset\"\n",
        "\n",
        "# Setting up directory structure\n",
        "Path().joinpath(dataset_name, \"validation\").mkdir(parents=True, exist_ok=True)\n",
        "Path().joinpath(dataset_name, \"training\", \"NO_PERSON\").mkdir(parents=True, exist_ok=True)\n",
        "Path().joinpath(dataset_name, \"training\", \"ALL_THE_PEOPLE\").mkdir(parents=True, exist_ok=True)\n",
        "Path().joinpath(dataset_name, \"training\", \"SOMEONE\").mkdir(parents=True, exist_ok=True)\n",
        "Path().joinpath(dataset_name, \"validation\", \"NO_PERSON\").mkdir(parents=True, exist_ok=True)\n",
        "Path().joinpath(dataset_name, \"validation\", \"ALL_THE_PEOPLE\").mkdir(parents=True, exist_ok=True)\n",
        "Path().joinpath(dataset_name, \"validation\", \"SOMEONE\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Files are moved from the training directory to the corresponding folders\n",
        "# both for training and for validation\n",
        "with open(str(Path().joinpath(dataset_name, target_file_name))) as f:\n",
        "    data = json.load(f)\n",
        "    pictures = list(data.keys())\n",
        "    random.shuffle(pictures)\n",
        "    validation_pictures = pictures[0:test_pictures_n]\n",
        "    for path in Path().joinpath(dataset_name, \"training\").glob(\"*.jpg\"):\n",
        "        if path.name in validation_pictures:\n",
        "            file_destination = str(Path().joinpath(dataset_name, \"validation\", path.name))\n",
        "            path.rename(file_destination)\n",
        "            path = Path(file_destination)\n",
        "        if data[path.name] == 0:\n",
        "            path.rename(str(Path(path.parent).joinpath(\"NO_PERSON\", path.name)))\n",
        "        elif data[path.name] == 1:\n",
        "            path.rename(str(Path(path.parent).joinpath(\"ALL_THE_PEOPLE\", path.name)))\n",
        "        elif data[path.name] == 2:\n",
        "            path.rename(str(Path(path.parent).joinpath(\"SOMEONE\", path.name)))\n",
        "        else:\n",
        "            raise ValueError(\"Unrecognized label in \" + target_file_name + \" allowed values are 0, 1, 2 found: \" + str(data[path.name]))\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJMJDpyYjyYA"
      },
      "source": [
        "# Imports and Random seed setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BkaP8GijyYC"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, AveragePooling2D, DepthwiseConv2D, LeakyReLU\n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "import scipy as sp\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "tf.random.set_seed(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41tFMUvNjyYH"
      },
      "source": [
        "# Dataset setup: augmentation and batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6YuN_dHjyYI",
        "outputId": "6efd6012-0baf-4ce4-a30c-9bc88f867b11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Batch Size\n",
        "bs = 32\n",
        "\n",
        "# Target Image Shape (max 358 x 256)\n",
        "img_h = 256\n",
        "img_w = 256\n",
        "\n",
        "# this is the augmentation configuration we will use for training\n",
        "train_data_gen = ImageDataGenerator(\n",
        "                                     rescale=1./255,\n",
        "                                     zoom_range=1,\n",
        "                                     channel_shift_range = 30,\n",
        "                                     horizontal_flip = True\n",
        "                                    )\n",
        "\n",
        "\n",
        "valid_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "dataset_dir = Path().joinpath(dataset_name)\n",
        "\n",
        "num_classes = 3\n",
        "classes = [\"NO_PERSON\",\n",
        "          \"ALL_THE_PEOPLE\",\n",
        "          \"SOMEONE\"]\n",
        "\n",
        "training_dir = dataset_dir.joinpath(\"training\")\n",
        "train_gen = train_data_gen.flow_from_directory(str(training_dir),\n",
        "                                              batch_size=bs,\n",
        "                                              classes=classes,\n",
        "                                              class_mode=\"categorical\",\n",
        "                                              shuffle=True,\n",
        "                                              target_size=(img_h, img_w),\n",
        "                                              seed=SEED)\n",
        "\n",
        "validation_dir = dataset_dir.joinpath(\"validation\")\n",
        "valid_gen = valid_data_gen.flow_from_directory(str(validation_dir),\n",
        "                                              batch_size=bs,\n",
        "                                              classes=classes,\n",
        "                                              class_mode=\"categorical\",\n",
        "                                              shuffle=True,\n",
        "                                              target_size=(img_h, img_w),\n",
        "                                              seed=SEED)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
        "                                              output_types=(tf.float32, tf.float32),\n",
        "                                              output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
        "                                              output_types=(tf.float32, tf.float32),\n",
        "                                              output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "valid_dataset = valid_dataset.repeat()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3930 images belonging to 3 classes.\n",
            "Found 1684 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r53GH4F8jyYR"
      },
      "source": [
        "# Class Weighting\n",
        "In the given datasets classes are not represented with an equal amount of samples, weighting deals with such issue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O96gZ_BJjyYT"
      },
      "source": [
        "from collections import Counter\n",
        "itemCt = Counter(train_gen.classes)\n",
        "maxCt = float(max(itemCt.values()))\n",
        "class_weight = {clsID : maxCt/numImg for clsID, numImg in itemCt.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRdo8h2cjyYZ"
      },
      "source": [
        "## LeNet+: LeNet with 1 convolutional layer more and ReLu\n",
        "LaNet is a simple model originally conceived for handwritten digits classification. The problem at hand is far harder thus a slightly more complex network has been chosen as a starting point. ReLu was not used either in LaNet but as of todays proves to be a very solid choice as an activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QghTKcDFjyYb"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# First Convolution\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(img_h, img_w, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second Convolution\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Third Convolution\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(64))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=num_classes, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XLZr81GjyYb"
      },
      "source": [
        "# LeNetb: LeNet+ with Batch Normalization\n",
        "TODO: retrain this with no dropout, may be the answer to validation instability (should still not work incredibly well)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBiNGoQRjyYc"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# First Convolution\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(img_h, img_w, 3)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Second Convolution\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Third Convolution\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
        "model.add(Dense(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(units=num_classes, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "epochs = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2aar8gN0jcm"
      },
      "source": [
        "# LeffeNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WyrXWBE06Bg"
      },
      "source": [
        "def get_block(model, size):\n",
        "  model.add(Conv2D(size, kernel_size=(3, 3), padding=\"same\", use_bias=False))\n",
        "  model.add(Activation(\"relu\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(tf.keras.layers.Input(shape=(img_h, img_w, 3)))\n",
        "get_block(model, 32)\n",
        "get_block(model, 64)\n",
        "get_block(model, 128)\n",
        "model.add(Conv2D(256, kernel_size=(3, 3)))\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation=\"relu\"))\n",
        "model.add(Dense(256, activation=\"relu\"))\n",
        "model.add(Dense(units=num_classes, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSSiWNiVjyYd"
      },
      "source": [
        "# Xception Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMMPvs1DjyYf"
      },
      "source": [
        "# Xception richiede input normalizzati in ingresso (procedura già svolta in ImageDataGenerator)\n",
        "base_model = tf.keras.applications.Xception(\n",
        "    weights='imagenet',\n",
        "    input_shape=(img_h, img_w, 3),\n",
        "    include_top=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAKFWDeqjyYj"
      },
      "source": [
        "# tlX: transfer learning with Xception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ogID8objyYk"
      },
      "source": [
        "base_model.trainable = False\n",
        "inputs = tf.keras.Input(shape=(img_h, img_w, 3))\n",
        "x = base_model(inputs, training=False) # training flag previene l'aggiornamento dei layer di Batch Normalization (importante per il fine tuning)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "outputs = Dense(units=num_classes, activation=\"softmax\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"]) \n",
        "epochs = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2qAR7iMjyYm"
      },
      "source": [
        "# ftX: fine tuning with Xception"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-3EB4ETjyYn"
      },
      "source": [
        "if not colab:\n",
        "    model = tf.keras.models.load_model(Path().joinpath(\"Results\", \"tlX.h5\"))\n",
        "else:\n",
        "    model = tf.keras.models.load_model(\"/content/Results/tlX.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZxl_rxzjyYo"
      },
      "source": [
        "# ChennaiNet: improving tlX\n",
        "Using tlX and ftX as a starting point the same training procedure is applied but the pooling is changed, a flattening layer and a dense layer with 0.5 Dropout are added taking inspiration from https://arxiv.org/pdf/2009.08369.pdf ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDGsYA1ljyYp"
      },
      "source": [
        "base_model.trainable = False\n",
        "inputs = tf.keras.Input(shape=(img_h, img_w, 3))\n",
        "x = base_model(inputs, training=False) # training flag previene l'aggiornamento dei layer di Batch Normalization (importante per il fine tuning)\n",
        "x = AveragePooling2D(pool_size=(5, 5))(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(128, kernel_constraint=max_norm(4.))(x)\n",
        "x = Activation(\"relu\")\n",
        "x = Dropout(0.5)\n",
        "x = Dense(units=num_classes, activation=\"softmax\") \n",
        "\n",
        "\n",
        "outputs = Dense(units=num_classes, activation=\"softmax\")(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"]) \n",
        "epochs = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6uXEQjqjyYq"
      },
      "source": [
        "if not colab:\n",
        "    model = tf.keras.models.load_model(Path().joinpath(\"Results\", \"chennai_net.h5\"))\n",
        "else:\n",
        "    model = tf.keras.models.load_model(\"/content/Results/chennai_net.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WQqbQ82jyYs"
      },
      "source": [
        "# KadapaNet: more parameters and back to global pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQclDFh5jyYt"
      },
      "source": [
        "base_model.trainable = False \n",
        "inputs = tf.keras.Input(shape=(img_h, img_w, 3))\n",
        "x = base_model(inputs, training=False) # training flag previene l'aggiornamento dei layer di Batch Normalization (importante per il fine tuning)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(2048, activation=\"relu\")(x)\n",
        "  \n",
        "outputs = Dense(units=num_classes, activation=\"softmax\")(x)\n",
        "epochs = 40\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer= \"adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcfM1QOEjyYy"
      },
      "source": [
        "if not colab:\n",
        "    model = tf.keras.models.load_model(Path().joinpath(\"Results\", \"kadapa_net.h5\"))\n",
        "else:\n",
        "    model = tf.keras.models.load_model(\"/content/Results/kadapa_net.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_k48w7BjyYz"
      },
      "source": [
        "# LebakaNet: one more dense layer\n",
        "This is the classifier structure used in the Xception paper https://arxiv.org/abs/1610.02357, Dropout layers are not present understand if adding them is needed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQVly9adjyYz"
      },
      "source": [
        "base_model.trainable = False \n",
        "inputs = tf.keras.Input(shape=(img_h, img_w, 3))\n",
        "x = base_model(inputs, training=False) # training flag previene l'aggiornamento dei layer di Batch Normalization (importante per il fine tuning)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(2048, activation=\"relu\")(x)\n",
        "# x = Dropout(0)\n",
        "x = Dense(2048, activation=\"relu\")(x)\n",
        "# x = Dropout(0)\n",
        "  \n",
        "outputs = Dense(units=num_classes, activation=\"softmax\")(x)\n",
        "epochs = 40\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(optimizer= \"adam\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJH-cHYYjyY7"
      },
      "source": [
        "# Set re-loaded transfer model to trainable for fine tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOrtJBlejyY-"
      },
      "source": [
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.python.keras.engine.training.Model):\n",
        "        base_model = layer\n",
        "base_model.trainable = True\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),  # Low learning rate\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXwrVcDOjyZB"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHp0uXeojyZB"
      },
      "source": [
        "callbacks_dir = Path().joinpath(\"Callbacks\")\n",
        "callbacks_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "now = datetime.now().strftime(\"%b%d_%H-%M-%S\")\n",
        "\n",
        "model_name = \"CNN\"\n",
        "\n",
        "callback_dir = callbacks_dir.joinpath(model_name + '_' + str(now))\n",
        "callback_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "callbacks = []\n",
        "\n",
        "# Model checkpoint\n",
        "ckpt_dir = callback_dir.joinpath(\"ckpts\")\n",
        "ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=str(ckpt_dir.joinpath(\"cp.ckpt\")), \n",
        "                                                   save_weights_only=True)\n",
        "callbacks.append(ckpt_callback)\n",
        "\n",
        "# Visualize Learning on Tensorboard\n",
        "tb_dir = callback_dir.joinpath(\"tb_logs\")\n",
        "tb_dir.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "tb_callback = tf.keras.callbacks.TensorBoard(log_dir=str(tb_dir),\n",
        "                                             profile_batch=0,\n",
        "                                             histogram_freq=1) \n",
        "callbacks.append(tb_callback)\n",
        "\n",
        "# Early Stopping\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    callbacks.append(es_callback)\n",
        "    \n",
        "# Learning Rate Annhealing\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=0, factor=0.5, min_lr=0.00001)\n",
        "lr_annhealing = True\n",
        "\n",
        "if lr_annhealing:\n",
        "    callbacks.append(learning_rate_reduction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MlMuiD5jyZG"
      },
      "source": [
        "# Tensorboard\n",
        "After running the cell below open [this link](http://localhost:6009) to view Tensorboard on a full browser page"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj9FJxUBjyZH",
        "outputId": "3bb1e3fe-e4c1-4ff8-8374-5975bae6a666"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir ./Callbacks --port 6009"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-9a943befa7c9629e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-9a943befa7c9629e\");\n",
              "          const url = new URL(\"/\", window.location);\n",
              "          const port = 6009;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLA5CSBXjyZP"
      },
      "source": [
        "# Model Fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RomnmHU4jyZQ",
        "outputId": "ddaf6869-ce84-4060-ab7d-7f3c329c1bfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "model.fit(x=train_dataset,\n",
        "          epochs=epochs,\n",
        "          steps_per_epoch=len(train_gen),\n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=len(valid_gen), \n",
        "          callbacks=callbacks,\n",
        "          class_weight = class_weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "123/123 [==============================] - 88s 719ms/step - loss: 0.7793 - accuracy: 0.6188 - val_loss: 0.6836 - val_accuracy: 0.6746\n",
            "Epoch 2/100\n",
            "123/123 [==============================] - 89s 723ms/step - loss: 0.7809 - accuracy: 0.6252 - val_loss: 0.6824 - val_accuracy: 0.6698\n",
            "Epoch 3/100\n",
            "123/123 [==============================] - 88s 719ms/step - loss: 0.8000 - accuracy: 0.6181 - val_loss: 0.6908 - val_accuracy: 0.6651\n",
            "Epoch 4/100\n",
            "123/123 [==============================] - 88s 718ms/step - loss: 0.7898 - accuracy: 0.6313 - val_loss: 0.6931 - val_accuracy: 0.6651\n",
            "Epoch 5/100\n",
            "123/123 [==============================] - 88s 717ms/step - loss: 0.7934 - accuracy: 0.6127 - val_loss: 0.6947 - val_accuracy: 0.6639\n",
            "Epoch 6/100\n",
            "123/123 [==============================] - 88s 715ms/step - loss: 0.7797 - accuracy: 0.6379 - val_loss: 0.6974 - val_accuracy: 0.6633\n",
            "Epoch 7/100\n",
            "123/123 [==============================] - 88s 713ms/step - loss: 0.7807 - accuracy: 0.6244 - val_loss: 0.7009 - val_accuracy: 0.6609\n",
            "Epoch 8/100\n",
            "123/123 [==============================] - 88s 715ms/step - loss: 0.7959 - accuracy: 0.6206 - val_loss: 0.6834 - val_accuracy: 0.6698\n",
            "Epoch 9/100\n",
            "123/123 [==============================] - 87s 710ms/step - loss: 0.7851 - accuracy: 0.6280 - val_loss: 0.7024 - val_accuracy: 0.6609\n",
            "Epoch 10/100\n",
            "123/123 [==============================] - 87s 711ms/step - loss: 0.7723 - accuracy: 0.6346 - val_loss: 0.6783 - val_accuracy: 0.6746\n",
            "Epoch 11/100\n",
            " 62/123 [==============>...............] - ETA: 37s - loss: 0.7982 - accuracy: 0.6087"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-e14c38bb9339>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m           class_weight = class_weight)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re7DN1uVjyZV"
      },
      "source": [
        "model.save(Path().joinpath(\"Results\", \"model\" + datetime.now().strftime('%b%d_%H-%M-%S')+ \".h5\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t__RPKBvjyZY"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIbagjh3jyZZ"
      },
      "source": [
        "## Test Time Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xSq-0r0jyZZ"
      },
      "source": [
        "tta = False\n",
        "\n",
        "def flip_lr(images):\n",
        "    return np.flip(images, axis=2)\n",
        "\n",
        "def shift(images, shift, axis):\n",
        "    return np.roll(images, shift, axis=axis)\n",
        "\n",
        "def rotate(images, angle):\n",
        "    return sp.ndimage.rotate(\n",
        "        images, angle, axes=(1,2),\n",
        "        reshape=False, mode='nearest')\n",
        "\n",
        "def combine_predictions(predictions):\n",
        "    pred_agg = np.mean(predictions, axis=0)\n",
        "    preds = np.argmax(pred_agg, axis=-1)\n",
        "    return preds\n",
        "\n",
        "def tta_predict(m, x_test):\n",
        "    pred = m.predict(x_test)\n",
        "\n",
        "    pred_f = m.predict(flip_lr(x_test))\n",
        "\n",
        "    pred_w0 = m.predict(shift(x_test, -3, axis=2))\n",
        "    pred_w1 = m.predict(shift(x_test, 3, axis=2))\n",
        "\n",
        "    pred_h0 = m.predict(shift(x_test, -3, axis=1))\n",
        "    pred_h1 = m.predict(shift(x_test, 3, axis=1))\n",
        "\n",
        "    pred_r0 = m.predict(rotate(x_test, -10))\n",
        "    pred_r1 = m.predict(rotate(x_test, 10))\n",
        "    # out = combine_predictions(np.stack((pred, pred_h0, pred_h1, pred_w0, pred_w1, pred_f, pred_r0, pred_r1)))\n",
        "    out = combine_predictions(np.stack((pred, pred_h0, pred_h1, pred_w0, pred_w1, pred_f)))\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arnGh-5pjyZg"
      },
      "source": [
        "results = {}\n",
        "for path in Path().joinpath(dataset_name, \"test\").glob(\"*.jpg\"):\n",
        "    image = Image.open(str(path)).convert(\"RGB\")\n",
        "    image = image.resize((img_w, img_h), Image.ANTIALIAS)\n",
        "    image = np.array(image)\n",
        "    image = np.expand_dims(image, 0)\n",
        "    image = np.float32(image) / 255.0\n",
        "    if not tta:\n",
        "        results[path.name]= model.predict(image).argmax(axis=-1)[0] \n",
        "    else:\n",
        "        results[path.name] = tta_predict(model, image)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7NcOcd6jyZj"
      },
      "source": [
        "csv_fname = \"results_\" + datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "Path().joinpath(\"Results\").mkdir(parents=True, exist_ok=True)\n",
        "with open(Path().joinpath(\"Results\", csv_fname), \"w\") as f:\n",
        "    f.write(\"Id,Category\\n\")\n",
        "    for key, value in results.items():\n",
        "        f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}